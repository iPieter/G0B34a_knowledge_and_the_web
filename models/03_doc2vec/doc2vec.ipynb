{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Train and Test\n",
    "The idea is to ultimately create a module that takes the data frame and return, instead of the body text, retuns a vector for each paragraph input (data input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train and test data\n",
    "\n",
    "* Data (train and test): [Cleaned reddit dataset](../../data/ad_hominem/ad_hominems_cleaned.csv), the data will be separated into test and train in a 70-30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "data = pd.read_csv(\"../../data/ad_hominem/ad_hominems_cleaned.csv\")\n",
    "train_data, test_data = np.split(data.sample(frac=1), [int(.7*len(data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Read and Preprocess Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a function to open the train/test file (with latin encoding)\n",
    "* Read the file line-by-line\n",
    "* Pre-process each line using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "* Return a list of words.\n",
    "Note that, for the data frame (corpus), each row constitutes a single document and the length of row entry (i.e., document) can vary. Also, to train the model, we'll need to associate a tag/number with each document of the training corpus. In our case, the tag is simply the index for the data frame (row number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(df, tokens_only=False):\n",
    "    #with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "    for i, line in df.iterrows():\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(str(line[\"reddit_ad_hominem.body\"]))\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(str(line[\"reddit_ad_hominem.body\"])), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(train_data))\n",
    "test_corpus = list(read_corpus(test_data, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus (both in the data frame and the generated corpus to see the differences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_ad_hominem.body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8182</th>\n",
       "      <td>I've never used 4Chan and from the little things I've seen from there over the years, I never will.From my understanding, 4Chan is the place for the undesirable anomalies of society to group together anonymously, mock the socially successful (or even unsuccessful), make jokes nobody finds funny for the sake of having inside jokes and say offensive things for the sake of them being offensive. All while being on a website created after 2Chan, a less so but still ridiculous Japanese version.The website looks like something from 2002 and is like a more confusing and pointless version of Reddit. I hate when people refer to the place and part of me thinks it's full of Neo-Nazi types, pedophiles and weebs. Like all these people desperately want to fit in somewhere but the rest of society hates them.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24828</th>\n",
       "      <td>it is a very real threat. There are parties actively pushing for more restrictions on gun ownership</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     reddit_ad_hominem.body\n",
       "8182   I've never used 4Chan and from the little things I've seen from there over the years, I never will.From my understanding, 4Chan is the place for the undesirable anomalies of society to group together anonymously, mock the socially successful (or even unsuccessful), make jokes nobody finds funny for the sake of having inside jokes and say offensive things for the sake of them being offensive. All while being on a website created after 2Chan, a less so but still ridiculous Japanese version.The website looks like something from 2002 and is like a more confusing and pointless version of Reddit. I hate when people refer to the place and part of me thinks it's full of Neo-Nazi types, pedophiles and weebs. Like all these people desperately want to fit in somewhere but the rest of society hates them. \n",
       "24828   it is a very real threat. There are parties actively pushing for more restrictions on gun ownership                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.DataFrame(train_data.loc[:, \"reddit_ad_hominem.body\"])[:2] #4323660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['ve', 'never', 'used', 'chan', 'and', 'from', 'the', 'little', 'things', 've', 'seen', 'from', 'there', 'over', 'the', 'years', 'never', 'will', 'from', 'my', 'understanding', 'chan', 'is', 'the', 'place', 'for', 'the', 'undesirable', 'anomalies', 'of', 'society', 'to', 'group', 'together', 'anonymously', 'mock', 'the', 'socially', 'successful', 'or', 'even', 'unsuccessful', 'make', 'jokes', 'nobody', 'finds', 'funny', 'for', 'the', 'sake', 'of', 'having', 'inside', 'jokes', 'and', 'say', 'offensive', 'things', 'for', 'the', 'sake', 'of', 'them', 'being', 'offensive', 'all', 'while', 'being', 'on', 'website', 'created', 'after', 'chan', 'less', 'so', 'but', 'still', 'ridiculous', 'japanese', 'version', 'the', 'website', 'looks', 'like', 'something', 'from', 'and', 'is', 'like', 'more', 'confusing', 'and', 'pointless', 'version', 'of', 'reddit', 'hate', 'when', 'people', 'refer', 'to', 'the', 'place', 'and', 'part', 'of', 'me', 'thinks', 'it', 'full', 'of', 'neo', 'nazi', 'types', 'pedophiles', 'and', 'weebs', 'like', 'all', 'these', 'people', 'desperately', 'want', 'to', 'fit', 'in', 'somewhere', 'but', 'the', 'rest', 'of', 'society', 'hates', 'them'], tags=[8182]),\n",
       " TaggedDocument(words=['it', 'is', 'very', 'real', 'threat', 'there', 'are', 'parties', 'actively', 'pushing', 'for', 'more', 'restrictions', 'on', 'gun', 'ownership'], tags=[24828])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_ad_hominem.body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22538</th>\n",
       "      <td>and more having to with the fact that transgender people are a very small minority in a society not designed for them in general.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>\"why don't you give that project to Veronica. She makes more money than I do, let her handle it.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                    reddit_ad_hominem.body\n",
       "22538   and more having to with the fact that transgender people are a very small minority in a society not designed for them in general. \n",
       "5969   \"why don't you give that project to Veronica. She makes more money than I do, let her handle it.\"                                  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_data.loc[:, \"reddit_ad_hominem.body\"])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['and', 'more', 'having', 'to', 'with', 'the', 'fact', 'that', 'transgender', 'people', 'are', 'very', 'small', 'minority', 'in', 'society', 'not', 'designed', 'for', 'them', 'in', 'general'], ['why', 'don', 'you', 'give', 'that', 'project', 'to', 'veronica', 'she', 'makes', 'more', 'money', 'than', 'do', 'let', 'her', 'handle', 'it']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain any tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "### Instantiate a Doc2Vec Object \n",
    "Doc2Vec model with:\n",
    "* Vector size with 500 words\n",
    "* Iterating over the training corpus 10 times (More iterations take more time and eventually reach a point of diminishing returns)\n",
    "* Minimum word count set to 20 (discard words with very few occurrences)\n",
    "\n",
    "Note: retaining infrequent words can often make a model worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=500, min_count=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via `model.wv.vocab`) of all of the unique words extracted from the training corpus along with the count (e.g., `model.wv.vocab['penalty'].count` for counts for the word `penalty`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 1.81 s, total: 2min 45s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vector can then be compared with other vectors via cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.10099314e-03,  1.96408131e-03,  4.54725325e-03,  9.16318968e-03,\n",
       "        1.16502456e-02,  2.16513555e-02,  4.69636125e-03,  3.01834126e-03,\n",
       "       -1.73208881e-02,  1.19826598e-02, -9.12664831e-03, -1.61856525e-02,\n",
       "        1.79378670e-02, -5.63165639e-03, -8.99874067e-05, -2.20483784e-02,\n",
       "        1.42881665e-02,  3.41686308e-02,  2.00273059e-02, -1.02976719e-02,\n",
       "        2.52445228e-02,  2.67276932e-02, -2.76128724e-02,  1.86041202e-02,\n",
       "       -4.02392671e-02,  1.81042142e-02,  5.15680127e-02, -1.04470998e-02,\n",
       "       -4.18031663e-02,  2.97561195e-03,  2.16589738e-02,  8.55786074e-03,\n",
       "        1.16063291e-02,  1.87184606e-02, -2.73608211e-02, -1.20737310e-02,\n",
       "       -3.78828496e-02, -1.63631998e-02, -5.23256622e-02,  4.22422169e-03,\n",
       "        2.63618119e-02, -1.08182644e-02,  7.80457119e-03,  2.88221240e-02,\n",
       "       -4.92784707e-03, -2.00535599e-02, -3.76831442e-02,  4.88839857e-02,\n",
       "       -2.74876226e-02, -2.54667606e-02,  1.58868032e-03, -8.84599425e-03,\n",
       "       -3.15850275e-03, -6.04691310e-03, -4.59071174e-02, -2.41218526e-02,\n",
       "        3.07416264e-02,  1.40868193e-02, -4.91729341e-02, -2.27981508e-02,\n",
       "       -4.98821400e-02, -2.79002041e-02,  4.65000188e-03,  1.64271146e-03,\n",
       "       -1.99566241e-02, -4.87204567e-02,  1.81257911e-02,  3.99813056e-02,\n",
       "       -1.97210051e-02,  3.63399088e-02, -1.36638666e-02,  1.23382090e-02,\n",
       "       -1.86053698e-03, -2.21032556e-03, -4.11266054e-04,  1.63193978e-02,\n",
       "       -5.66789806e-02, -1.73129179e-02, -9.83720552e-03, -4.46508005e-02,\n",
       "        4.04148065e-02,  1.38659822e-02, -1.53809218e-02, -2.93329470e-02,\n",
       "        3.60373198e-03,  4.59174737e-02, -1.34614464e-02,  2.20059101e-02,\n",
       "       -1.08095258e-02, -3.24091725e-02,  3.79034728e-02, -8.25034454e-03,\n",
       "       -2.26965491e-02, -1.82774849e-02,  1.49861083e-03,  4.97966306e-03,\n",
       "        6.85938029e-03, -1.89653710e-02,  3.32340337e-02,  9.63579863e-03,\n",
       "       -3.22696473e-03,  6.71139802e-04,  2.12263851e-03, -8.33199024e-02,\n",
       "        4.38193837e-03,  1.03470800e-03,  3.58802155e-02,  2.23260224e-02,\n",
       "       -1.35031659e-02,  2.11370029e-02, -7.47281034e-03, -3.35118920e-03,\n",
       "        3.01548094e-02, -5.54015907e-03, -2.73114834e-02, -3.44665311e-02,\n",
       "       -5.14938608e-02, -5.22394851e-02,  2.06451416e-02, -4.76541184e-03,\n",
       "        1.46618867e-02,  2.62699109e-02, -1.16217537e-02, -2.87265647e-02,\n",
       "       -8.47259944e-04, -6.59929812e-02, -1.17691755e-02, -9.77334380e-03,\n",
       "       -4.20653820e-02, -2.96217995e-03, -5.39194234e-03,  3.40599306e-02,\n",
       "        4.80816104e-02,  5.15746102e-02, -4.82069924e-02,  2.97768526e-02,\n",
       "        2.86698863e-02,  6.70114357e-04, -1.81949120e-02, -2.19712853e-02,\n",
       "        1.25209917e-03, -1.96748022e-02,  1.33427512e-03, -1.36845829e-02,\n",
       "       -3.04995757e-02,  1.96633730e-02,  5.61473286e-03, -1.53169613e-02,\n",
       "        2.26361342e-02,  2.96784341e-02,  6.49397401e-03,  6.87437551e-03,\n",
       "       -3.65762319e-03,  2.41882708e-02,  7.49199139e-03, -6.72139181e-03,\n",
       "       -1.96348876e-02, -2.67033074e-02, -2.82555688e-02,  1.81122106e-02,\n",
       "       -1.16948228e-04,  3.82570587e-02,  1.41986785e-02,  2.85266656e-02,\n",
       "       -1.37632862e-02,  9.77078546e-03,  2.48230994e-02,  1.27596455e-02,\n",
       "        1.37533778e-02, -4.09062058e-02,  1.84844655e-03, -3.44526619e-02,\n",
       "        3.36144120e-02,  1.29817594e-02,  1.62740443e-02,  1.74252186e-02,\n",
       "       -3.64777930e-02,  3.35079432e-02,  3.72792371e-02, -2.37319916e-02,\n",
       "        6.94917107e-04,  1.33061865e-02,  5.03614023e-02, -5.31444624e-02,\n",
       "       -2.18817983e-02, -1.36703318e-02, -2.98954290e-03,  1.20070833e-03,\n",
       "       -2.78569870e-02, -7.41533842e-03,  1.83607005e-02,  3.98035645e-02,\n",
       "        4.52579595e-02,  9.38945275e-04,  2.84562279e-02, -1.43567529e-02,\n",
       "       -4.58870530e-02,  5.37257604e-02, -1.86446402e-02,  3.12159453e-02,\n",
       "       -1.16621051e-02, -3.93223017e-02,  4.75245118e-02, -1.07229082e-02,\n",
       "        1.63145643e-02, -6.31892635e-03,  2.12306492e-02,  6.31781146e-02,\n",
       "       -1.00144530e-02,  4.40059863e-02, -3.99844395e-03, -6.26752619e-04,\n",
       "        4.28548083e-02,  2.15356927e-02,  4.50332798e-02, -2.14845501e-02,\n",
       "        3.00114751e-02, -1.89794472e-03, -7.92768784e-03, -6.19274117e-02,\n",
       "        4.75429855e-02, -5.29558025e-02, -3.42843942e-02,  3.32961720e-03,\n",
       "        3.98543179e-02,  2.35204734e-02,  2.98179071e-02,  6.84325863e-03,\n",
       "       -3.44158220e-03,  4.30739149e-02,  6.64293692e-02, -2.31317598e-02,\n",
       "        5.01191644e-05,  2.18846519e-02, -5.28417574e-03,  7.57631008e-03,\n",
       "       -8.33634846e-03, -1.72968977e-03, -4.37726919e-03,  1.04341647e-02,\n",
       "       -4.16041017e-02,  8.16195272e-03, -7.28057639e-04,  6.50030896e-02,\n",
       "        6.08609766e-02,  3.93826291e-02,  6.01623766e-02,  4.76694070e-02,\n",
       "       -7.50512853e-02,  7.44009344e-03, -2.71052076e-03, -4.49395217e-02,\n",
       "       -9.69521701e-03, -2.39674300e-02, -3.72430086e-02,  4.74085193e-03,\n",
       "       -3.69674689e-03,  5.44310808e-02, -1.43839307e-02,  1.95504003e-03,\n",
       "       -7.87766278e-03,  4.13768413e-03,  2.69929562e-02,  2.59256084e-02,\n",
       "        5.40637672e-02,  7.07760407e-03, -6.64232224e-02,  1.49868643e-02,\n",
       "       -2.10035536e-02, -3.19031812e-02, -3.84617597e-02, -2.10951325e-02,\n",
       "       -1.29222916e-03,  2.73781847e-02,  2.33880114e-02,  4.07012105e-02,\n",
       "        1.37416935e-02, -2.29810998e-02,  1.55655881e-02, -3.26738656e-02,\n",
       "       -9.48343519e-03,  5.73471608e-03,  3.76945478e-03,  2.69229040e-02,\n",
       "        5.13699837e-02,  2.03331858e-02, -1.22423973e-02, -5.94745064e-03,\n",
       "       -2.73709092e-02, -5.73651586e-03, -6.87527144e-03, -1.66577322e-03,\n",
       "       -1.47605489e-03,  5.79010788e-03, -3.61441821e-02,  2.07135342e-02,\n",
       "        1.85388874e-03,  2.97913663e-02, -5.50884753e-03,  1.01719396e-02,\n",
       "       -6.67986879e-03,  6.42125402e-03,  3.92421894e-03,  1.14039946e-02,\n",
       "       -1.23189958e-02,  2.09246785e-03, -1.82716746e-03, -4.63325679e-02,\n",
       "       -1.30010555e-02,  1.15569693e-03,  1.88159605e-03,  2.95386110e-02,\n",
       "       -4.61853892e-02,  2.91080605e-02, -3.40939090e-02, -6.41456097e-02,\n",
       "       -2.08605677e-02,  4.77319695e-02,  8.17517564e-03,  4.43825237e-02,\n",
       "        4.29293960e-02, -1.39781032e-02,  5.33955991e-02, -6.07716991e-03,\n",
       "        2.12485772e-02, -4.82168123e-02,  2.47789249e-02,  1.62244420e-02,\n",
       "       -1.53847933e-02, -8.79774627e-04,  2.93776337e-02,  5.87321771e-03,\n",
       "        2.63027791e-02, -3.24034169e-02, -2.90072896e-02,  4.25859541e-02,\n",
       "       -1.28057925e-03, -4.98106331e-03, -4.79571335e-02,  4.23618294e-02,\n",
       "        6.36668876e-02, -8.10122117e-03,  1.98598788e-03, -3.85871045e-02,\n",
       "       -1.16015496e-02, -1.55375935e-02,  1.95268039e-02,  1.53676588e-02,\n",
       "        1.86132663e-03, -9.92212445e-03,  1.94523782e-02, -1.65900588e-02,\n",
       "       -5.36215752e-02, -3.12968232e-02,  4.45412695e-02,  1.65497307e-02,\n",
       "       -1.38600674e-02,  1.57542564e-02,  1.33382631e-02, -7.40510644e-03,\n",
       "        1.95680559e-02,  5.44301569e-02, -1.32201370e-02,  5.11971191e-02,\n",
       "       -3.20352800e-02,  5.08779958e-02,  2.21580975e-02,  2.89757606e-02,\n",
       "       -3.92197296e-02, -3.78464349e-02, -3.94771472e-02, -5.00515439e-02,\n",
       "       -4.08335738e-02,  9.85692628e-03,  4.04945314e-02, -1.15068555e-02,\n",
       "        1.09073948e-02, -2.36837305e-02, -1.19225457e-02, -1.34649863e-02,\n",
       "       -3.83999906e-02, -1.23332711e-02, -4.66664582e-02,  4.17482965e-02,\n",
       "        4.75672074e-03,  4.17843610e-02,  3.77182029e-02, -9.01062638e-02,\n",
       "       -1.26641924e-02,  1.74327288e-02, -4.68241796e-03,  9.76299308e-03,\n",
       "        1.12906918e-02, -1.74466036e-02, -3.58697250e-02,  1.13112004e-02,\n",
       "        5.10265008e-02, -3.33450101e-02,  2.83588991e-02,  7.23230792e-03,\n",
       "        1.53386835e-02,  1.02766585e-02,  1.46769611e-02,  1.87439099e-02,\n",
       "        3.28744166e-02, -1.39411362e-02,  1.74644981e-02, -1.66108012e-02,\n",
       "       -9.97697841e-03, -1.25883818e-01,  7.36301718e-03, -1.38456784e-02,\n",
       "        3.03756092e-02,  1.51968421e-02,  2.33314857e-02, -3.26459808e-03,\n",
       "       -4.04330576e-03,  2.58867331e-02,  1.77479330e-02, -6.42707665e-03,\n",
       "        2.27664877e-02, -3.41707990e-02,  1.23764956e-02,  9.17000324e-03,\n",
       "       -3.74550670e-02,  1.05498070e-02, -5.24711087e-02,  4.07804251e-02,\n",
       "       -2.07616538e-02,  1.66607299e-03,  1.34610615e-04,  3.50043103e-02,\n",
       "       -2.78215520e-02,  4.24018409e-03, -3.85420434e-02, -5.94946779e-02,\n",
       "       -2.15376238e-03, -4.35742885e-02,  4.29382995e-02,  8.80359933e-02,\n",
       "        1.27418227e-02, -6.00379100e-03,  5.29117398e-02,  3.35955136e-02,\n",
       "        5.85117526e-02,  6.91337138e-02, -1.36989579e-02, -5.57177775e-02,\n",
       "        2.57639978e-02, -3.36548593e-03,  3.14207077e-02, -3.37594142e-03,\n",
       "        5.30430973e-02,  3.29085514e-02, -4.16610315e-02,  5.02380496e-03,\n",
       "       -3.07019372e-02, -1.03474688e-03,  6.97014481e-03, -2.32522395e-02,\n",
       "        2.92237103e-02, -2.17807256e-02,  3.71371284e-02,  1.37458546e-02,\n",
       "        6.30579218e-02,  1.95095968e-03,  2.33787224e-02, -5.63973524e-02,\n",
       "        6.03116713e-02, -3.94268557e-02, -3.56925912e-02,  5.18486649e-02,\n",
       "       -3.02779507e-02,  4.21916172e-02, -2.79128104e-02, -1.35149844e-02,\n",
       "       -3.04777864e-02,  8.89569614e-03,  2.30718926e-02,  3.44865546e-02,\n",
       "       -1.29527431e-02,  2.40547080e-02,  8.63529742e-03, -2.41335854e-02,\n",
       "        9.47268866e-03,  3.32586057e-02, -1.62652954e-02, -2.44053565e-02,\n",
       "       -2.75088698e-02,  4.96294536e-02, -5.04479632e-02,  1.89411882e-02,\n",
       "       -3.12732384e-02,  4.23266143e-02, -4.05239463e-02, -4.75353226e-02,\n",
       "       -3.15419137e-02,  9.25380085e-03, -5.57854865e-03, -5.11734672e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `infer_vector()` takes a list of *string tokens*\n",
    "* Input should be tokenized prior to inference\n",
    "    * Here the test set is already tokenized (in `test_corpus = list(read_corpus(test_data, tokens_only=True))`)\n",
    "    \n",
    "Note: algorithms use internal randomization, so repeated inferences of the same text will return slightly different vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "Deleting training data from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"reddit-doc2vec.model\")\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load the model:\n",
    "\n",
    "`model = Doc2Vec.load(fname)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To use model for inference:\n",
    "\n",
    "`vector = model.infer_vector([\"tokenized\", \"input\", \"string\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "### To load the model:\n",
    "* `model = Doc2Vec.load(fname)` (not required here)\n",
    "### To use model for inference:\n",
    "* `vector = model.infer_vector([\"tokenized\", \"input\", \"string\"])`\n",
    "    #### To tokenize:\n",
    "    * `list(read_corpus(df, tokens_only=False))` (used earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized test sample:\n",
      "['why', 'don', 'you', 'give', 'that', 'project', 'to', 'veronica', 'she', 'makes', 'more', 'money', 'than', 'do', 'let', 'her', 'handle', 'it']\n",
      "\n",
      "Inferred vector:\n",
      "[-0.01985193  0.01519804  0.0105221  -0.02903783  0.04584349  0.00821724\n",
      "  0.01950787 -0.04702343 -0.04687892 -0.03594166 -0.01066448 -0.0942203\n",
      " -0.01465426 -0.01166661 -0.0058945   0.014545   -0.04101643  0.0406091\n",
      " -0.00967981 -0.00633664 -0.00180249 -0.05288441 -0.00942213  0.01729073\n",
      "  0.01860647  0.02358845  0.01816625 -0.02612522 -0.03782067 -0.07178894\n",
      "  0.07045363 -0.05475719 -0.04246257  0.00980603 -0.11037128  0.04370284\n",
      "  0.00813757  0.00931698  0.00326506  0.00490022  0.02962516 -0.01799162\n",
      "  0.01522217  0.00149615  0.00011974 -0.03393247  0.01070947 -0.00145847\n",
      " -0.03797978  0.00212131  0.01929183  0.03622057  0.0517694   0.04490124\n",
      " -0.02248931  0.0564543   0.03109168  0.03142186 -0.06635758 -0.01958945\n",
      " -0.01405963  0.05588077 -0.02224294  0.01246074  0.08489747  0.02165684\n",
      " -0.02622388 -0.04187012 -0.02658127  0.01127823  0.01982574  0.00173564\n",
      "  0.0239779  -0.00524524 -0.0077409   0.03579354 -0.01738223  0.07082776\n",
      " -0.00861667 -0.00785259  0.03769389 -0.02308302 -0.04302493 -0.03082043\n",
      "  0.01212176 -0.01087686  0.03639944 -0.00205412 -0.00250943 -0.08381526\n",
      "  0.07117682 -0.03448928 -0.00580419 -0.05599938  0.0284619   0.05889302\n",
      "  0.01472879 -0.0176618  -0.00435449 -0.0173637   0.07644105 -0.00615238\n",
      " -0.03559728 -0.07015755  0.06163624 -0.01248873  0.01065641  0.02510099\n",
      " -0.07208854 -0.00359478 -0.00735681  0.09694587  0.06261087 -0.00107865\n",
      " -0.01818832 -0.06236454 -0.08081579 -0.00332014  0.03265189 -0.01280973\n",
      "  0.01302113  0.02498202  0.09326293 -0.00767627 -0.01560994 -0.03283774\n",
      " -0.05820381 -0.00095697 -0.09136431 -0.03055917 -0.02595062  0.00598772\n",
      "  0.00774447  0.05480656 -0.02482416  0.08652874 -0.00061215  0.03715441\n",
      " -0.0566244  -0.053542   -0.03102168  0.00393907  0.0184539  -0.01998554\n",
      "  0.00838241  0.01355144 -0.04897717  0.03629898 -0.00328395  0.00892776\n",
      " -0.00203531  0.02747472 -0.01852798 -0.00861891  0.01682052 -0.02239477\n",
      " -0.034768   -0.00914202 -0.06236618 -0.0485225   0.00676107  0.06501951\n",
      "  0.01350038  0.030531    0.02786615  0.02803214  0.08887196 -0.01197241\n",
      " -0.08144273  0.01035573  0.00582396  0.01763998  0.04703584  0.01956866\n",
      " -0.06702623  0.01579902 -0.02308863  0.04694856 -0.04061961 -0.03473438\n",
      " -0.03260465 -0.06083128  0.05648443  0.11934782  0.03745776 -0.03538626\n",
      " -0.01516093  0.00541584 -0.02587441  0.02373312  0.04113099 -0.03150268\n",
      "  0.04391463  0.07394294  0.04075504  0.0331528   0.01519784  0.01394325\n",
      "  0.01771377 -0.04530006  0.01605879  0.00223881  0.02648892  0.0188341\n",
      "  0.02061554 -0.01741064  0.0395845   0.02878069  0.00502557  0.03607062\n",
      " -0.03245957  0.04047954  0.02543492  0.04889324 -0.02484872  0.02071961\n",
      " -0.00570385 -0.04697024  0.03081203  0.00313234  0.02025436 -0.06620182\n",
      " -0.01357849 -0.01251253  0.02141943  0.0518797   0.01110372  0.06111445\n",
      " -0.02590328 -0.01065764 -0.02378827  0.03256559  0.04277403  0.05439121\n",
      " -0.02164553 -0.01854455 -0.01657972 -0.03427581  0.01068242 -0.02533185\n",
      " -0.02596329  0.02246565 -0.00669113 -0.02169579  0.02735255  0.00391745\n",
      "  0.07365868  0.01620702  0.02406098 -0.02101374 -0.02362554 -0.01871094\n",
      "  0.00736833 -0.00917984 -0.09532227  0.06427038 -0.02833767  0.01611701\n",
      " -0.07544044 -0.02495342 -0.03013524 -0.05846577  0.01503385 -0.00413477\n",
      "  0.02031931  0.00524506  0.0480293  -0.02907688  0.03954744 -0.05739625\n",
      " -0.03001799 -0.04687422 -0.01142788  0.0366395   0.00869936 -0.0113989\n",
      " -0.03010669 -0.05067568 -0.02619948  0.01411914  0.01796405  0.06883159\n",
      "  0.03805974 -0.02350679  0.03615374  0.04456569 -0.04482146 -0.01904462\n",
      " -0.0479056   0.01279225  0.09655035  0.02850889 -0.04428745  0.04895072\n",
      " -0.00598408  0.00986928 -0.01578471 -0.00835273 -0.05429068 -0.03222105\n",
      " -0.01604577  0.05889187 -0.00439262  0.01470006 -0.07425852  0.02618348\n",
      " -0.03294602  0.01522589 -0.01175451 -0.00053966 -0.02322051  0.03314566\n",
      " -0.07560151  0.00028722 -0.02085464 -0.03363132 -0.04275582  0.03877513\n",
      "  0.03649372 -0.04768014 -0.03713192  0.00383266  0.05590391  0.05945891\n",
      " -0.06504205  0.01008543  0.06855744  0.03247096  0.07613056 -0.01363679\n",
      "  0.04860996  0.00791641 -0.05843944 -0.10116493 -0.0376875  -0.02841964\n",
      "  0.01262722  0.01016661 -0.02514236 -0.01600337 -0.01401191 -0.01219444\n",
      " -0.02489647 -0.0378626  -0.02024833  0.02325538 -0.0195436  -0.03270706\n",
      " -0.00691418 -0.00224149 -0.00301928  0.03432013 -0.04970741 -0.02571721\n",
      " -0.03687983  0.01181361 -0.02499546  0.00963735  0.01859421 -0.0202966\n",
      "  0.00150611 -0.00846213 -0.04664922  0.02978168  0.04057255  0.07371484\n",
      " -0.05952603 -0.01044446 -0.01365659  0.00986014 -0.01682856 -0.01952259\n",
      " -0.01609601  0.08268806 -0.01626347 -0.05136027 -0.01828118  0.03342474\n",
      "  0.00179496 -0.01834676 -0.04040785 -0.01225679  0.02533928 -0.02136459\n",
      " -0.01030108  0.00373163  0.01202705 -0.05445065 -0.03711461 -0.00084238\n",
      " -0.00055094 -0.01843568  0.00603925  0.05512967 -0.00321993 -0.06924009\n",
      "  0.02719397  0.00304459 -0.04270296  0.05452523 -0.06836443  0.00611119\n",
      "  0.0023831  -0.01016322  0.04635291  0.00953167 -0.03460093  0.0259718\n",
      "  0.01573422  0.01349572  0.04766949 -0.05046698 -0.06359189 -0.00451457\n",
      " -0.04468063 -0.0096421  -0.01435002 -0.01531752  0.0660871  -0.01540165\n",
      " -0.03557807 -0.00704443  0.01190897  0.03155314 -0.02157167  0.06393505\n",
      "  0.02866521 -0.00411556 -0.06234344  0.04190957  0.00985421  0.00799088\n",
      "  0.0294854  -0.09575729  0.01747629  0.0005162  -0.07087978 -0.02403958\n",
      "  0.03944338 -0.00302794  0.01991937  0.02845255 -0.00315965  0.02200327\n",
      "  0.03011206  0.04843273 -0.0404681   0.03465506  0.02276152  0.0275679\n",
      "  0.06746604 -0.05149025 -0.04363074  0.03295456 -0.06715099 -0.07899965\n",
      "  0.01308251  0.03992254 -0.05227261 -0.0372992   0.02709428 -0.05239566\n",
      "  0.08270159 -0.02333881 -0.01737537  0.02545506  0.02860311  0.00965102\n",
      " -0.02050971 -0.03624205 -0.06460173  0.09088312  0.03505777 -0.00223921\n",
      "  0.01224074  0.06817853  0.03724044 -0.01558151  0.01994778  0.10341072\n",
      " -0.05178506  0.04669573  0.0042237  -0.0335723   0.08441718 -0.03153091\n",
      " -0.04438439 -0.06125626 -0.02309196 -0.05354051 -0.0656412  -0.01272864\n",
      "  0.0026489  -0.05483929  0.04005569 -0.03229134 -0.04578997  0.04361495\n",
      " -0.04454828 -0.07976476]\n"
     ]
    }
   ],
   "source": [
    "vector_sample = model.infer_vector(test_corpus[1])\n",
    "print(\"Tokenized test sample:\")\n",
    "print(test_corpus[1])\n",
    "print(\"\\nInferred vector:\")\n",
    "print(vector_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For more:\n",
    "* [Yaron Vazana](http://yaronvazana.com/2018/01/20/training-doc2vec-model-with-gensim/)\n",
    "* [Rare Technologies](https://rare-technologies.com/doc2vec-tutorial/)\n",
    "* [Gensim Documentation](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "* [Gensim Doc2Vec Tutorial on the IMDB Sentiment Dataset](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb)\n",
    "* [Doc2Vec Tutorial on the Lee Dataset](https://markroxor.github.io/gensim/static/notebooks/doc2vec-lee.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
