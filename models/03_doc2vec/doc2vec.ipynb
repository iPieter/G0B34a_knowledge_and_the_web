{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Train and Test\n",
    "The idea is to ultimately create a module that takes the data frame and return, instead of the body text, retuns a vector for each paragraph input (data input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train and test data\n",
    "\n",
    "* Data (train and test): [Cleaned reddit dataset](../../data/ad_hominem/ad_hominems_cleaned_Murilo.csv), the data will be separated into test and train in a 70-30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "data = pd.read_csv(\"../../data/ad_hominem/ad_hominems_cleaned_Murilo.csv\")\n",
    "train_data, test_data = np.split(data.sample(frac=1), [int(.7*len(data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Read and Preprocess Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a function to open the train/test file (with latin encoding)\n",
    "* Read the file line-by-line\n",
    "* Pre-process each line using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "* Return a list of words.\n",
    "Note that, for the data frame (corpus), each row constitutes a single document and the length of row entry (i.e., document) can vary. Also, to train the model, we'll need to associate a tag/number with each document of the training corpus. In our case, the tag is simply the index for the data frame (row number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(df, tokens_only=False):\n",
    "    #with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "    for i, line in df.iterrows():\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(str(line[\"reddit_ad_hominem.body\"]))\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(str(line[\"reddit_ad_hominem.body\"])), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(train_data))\n",
    "test_corpus = list(read_corpus(test_data, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus (both in the data frame and the generated corpus to see the differences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_ad_hominem.body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26164</th>\n",
       "      <td>and 25.5 percent black.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18612</th>\n",
       "      <td>First of all, I'm not talking about veganism. There is a big difference between not eating red meat and being a vegan. Second of all, that thing about meat being a necessity for athletes is simply false, 100 %. About enjoying meat being a rational argument, you are right. I addressed that in one of my posts. That was a poor choice of words by me.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                              reddit_ad_hominem.body\n",
       "26164   and 25.5 percent black.                                                                                                                                                                                                                                                                                                                                     \n",
       "18612  First of all, I'm not talking about veganism. There is a big difference between not eating red meat and being a vegan. Second of all, that thing about meat being a necessity for athletes is simply false, 100 %. About enjoying meat being a rational argument, you are right. I addressed that in one of my posts. That was a poor choice of words by me. "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.DataFrame(train_data.loc[:, \"reddit_ad_hominem.body\"])[:2] #4323660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['and', 'percent', 'black'], tags=[26164]),\n",
       " TaggedDocument(words=['first', 'of', 'all', 'not', 'talking', 'about', 'veganism', 'there', 'is', 'big', 'difference', 'between', 'not', 'eating', 'red', 'meat', 'and', 'being', 'vegan', 'second', 'of', 'all', 'that', 'thing', 'about', 'meat', 'being', 'necessity', 'for', 'athletes', 'is', 'simply', 'false', 'about', 'enjoying', 'meat', 'being', 'rational', 'argument', 'you', 'are', 'right', 'addressed', 'that', 'in', 'one', 'of', 'my', 'posts', 'that', 'was', 'poor', 'choice', 'of', 'words', 'by', 'me'], tags=[18612])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_ad_hominem.body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20961</th>\n",
       "      <td>which is what we're talking about. That's why your air analogy is garbage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24385</th>\n",
       "      <td>”</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           reddit_ad_hominem.body\n",
       "20961   which is what we're talking about. That's why your air analogy is garbage\n",
       "24385  ”                                                                         "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_data.loc[:, \"reddit_ad_hominem.body\"])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['which', 'is', 'what', 'we', 're', 'talking', 'about', 'that', 'why', 'your', 'air', 'analogy', 'is', 'garbage'], []]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain any tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "### Instantiate a Doc2Vec Object \n",
    "Doc2Vec model with:\n",
    "* Vector size with 500 words\n",
    "* Iterating over the training corpus 10 times (More iterations take more time and eventually reach a point of diminishing returns)\n",
    "* Minimum word count set to 20 (discard words with very few occurrences)\n",
    "\n",
    "Note: retaining infrequent words can often make a model worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=500, min_count=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via `model.wv.vocab`) of all of the unique words extracted from the training corpus along with the count (e.g., `model.wv.vocab['penalty'].count` for counts for the word `penalty`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 42s, sys: 1.45 s, total: 2min 43s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vector can then be compared with other vectors via cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03143369,  0.0407154 ,  0.03831863,  0.05787762,  0.003495  ,\n",
       "       -0.01959528, -0.1392654 ,  0.02381854,  0.05999694,  0.00511141,\n",
       "        0.0614543 , -0.07074674, -0.03040167,  0.05974266, -0.05069724,\n",
       "       -0.0095914 ,  0.00403029,  0.01088104, -0.05114654, -0.01545902,\n",
       "       -0.00697675, -0.0191806 ,  0.01606508, -0.04181037,  0.04848306,\n",
       "       -0.00219237, -0.00888258, -0.01794555, -0.00313175, -0.00407065,\n",
       "        0.00936912, -0.02379045,  0.01550802,  0.05234915, -0.01517074,\n",
       "        0.02088564,  0.02866564,  0.00668709,  0.0375262 ,  0.00101556,\n",
       "       -0.02678772, -0.01674695, -0.03478102, -0.06777813,  0.03225682,\n",
       "       -0.03425853, -0.02280523, -0.01904884,  0.03232688, -0.01424852,\n",
       "       -0.01720729, -0.02564723,  0.00844694, -0.0238448 , -0.02616653,\n",
       "       -0.02288621,  0.00434677, -0.00772536,  0.02465179,  0.01039768,\n",
       "        0.00927351,  0.01873142,  0.0036584 ,  0.05478862,  0.0434427 ,\n",
       "        0.00309651,  0.01977954, -0.06521635, -0.03103744,  0.02630127,\n",
       "        0.0183502 , -0.02815388,  0.04413637,  0.05067652, -0.01227277,\n",
       "        0.02535917, -0.00869382, -0.02114496, -0.00412998, -0.04241867,\n",
       "       -0.01675836, -0.02245218, -0.03301107,  0.00279498,  0.00174138,\n",
       "       -0.00822575, -0.00307316,  0.01769746, -0.0132345 ,  0.00616105,\n",
       "       -0.01090499, -0.01392259, -0.03407563,  0.00221805, -0.00428108,\n",
       "        0.01283346, -0.02950612,  0.00474782,  0.00542359,  0.00248285,\n",
       "       -0.02070419, -0.05627155, -0.00085817, -0.00319071,  0.04218742,\n",
       "       -0.02995918, -0.02538378,  0.03616992, -0.00614909, -0.00447865,\n",
       "        0.00044711, -0.01593849,  0.01204896,  0.00300112, -0.01306995,\n",
       "       -0.01546058, -0.02398699,  0.01846624,  0.001378  ,  0.00742598,\n",
       "        0.04779479,  0.02725222,  0.01765502, -0.03720661, -0.02999739,\n",
       "       -0.00713847,  0.0061646 ,  0.00227829,  0.03133988,  0.00857803,\n",
       "       -0.03913432, -0.01547102,  0.00897706,  0.01322623,  0.0369704 ,\n",
       "       -0.02911061, -0.03709202,  0.01025916,  0.01388906, -0.00410357,\n",
       "       -0.02678465,  0.00612042, -0.02570025,  0.0142145 ,  0.02117995,\n",
       "        0.0036383 , -0.00101084, -0.00275221,  0.03010137,  0.00309074,\n",
       "        0.024067  ,  0.00223354,  0.0149392 ,  0.00388452, -0.04632752,\n",
       "        0.01265272,  0.04660187, -0.01551447,  0.01825982,  0.02100055,\n",
       "       -0.03418434,  0.02584481,  0.016653  , -0.02057601,  0.02236566,\n",
       "        0.02568399, -0.01615729, -0.00568589, -0.01799105,  0.00793971,\n",
       "        0.01026474, -0.02123937,  0.01668542,  0.04010531, -0.00707383,\n",
       "        0.03521895, -0.00062825, -0.03131802, -0.02013499,  0.04338667,\n",
       "       -0.0138213 ,  0.02152317, -0.00612176,  0.00289775, -0.00157439,\n",
       "        0.00603396,  0.00210553, -0.01746582,  0.01944204, -0.01456416,\n",
       "        0.04058189, -0.01557034,  0.00827691,  0.0365017 , -0.00550759,\n",
       "        0.02666395,  0.00125939,  0.02565683, -0.01358613, -0.02917224,\n",
       "        0.02694198, -0.0167163 , -0.01595559,  0.00294125, -0.00815284,\n",
       "       -0.01110031,  0.02380945, -0.01327753, -0.00512448,  0.03140322,\n",
       "        0.02831253, -0.00519263, -0.01293173, -0.02808687,  0.01934987,\n",
       "       -0.01023006, -0.00525392, -0.0106546 , -0.03846673,  0.00676954,\n",
       "        0.01839481, -0.00245122, -0.02546476, -0.02449319, -0.03514539,\n",
       "        0.00754349,  0.02350716,  0.00166892,  0.0215545 , -0.01553764,\n",
       "        0.04693778,  0.02341925, -0.02757491, -0.00539387, -0.0011637 ,\n",
       "        0.02305889, -0.04441258,  0.00204391, -0.00859792, -0.0374036 ,\n",
       "        0.00551377,  0.04242402, -0.00384138,  0.01031101, -0.01968716,\n",
       "        0.01297272, -0.02242284, -0.01618961, -0.00686078, -0.00375985,\n",
       "        0.01945937, -0.02085565, -0.03112811,  0.01061361, -0.0182784 ,\n",
       "       -0.01721667, -0.00915945, -0.00899943, -0.07612233, -0.04501722,\n",
       "        0.06505647,  0.03282676, -0.02950935,  0.05138398,  0.02668519,\n",
       "       -0.02510527, -0.01813427,  0.01639131,  0.02624796, -0.01919938,\n",
       "       -0.07965425,  0.00145842, -0.01550876,  0.009091  ,  0.05855806,\n",
       "       -0.00263609,  0.0375999 ,  0.01895295, -0.03104178,  0.04913297,\n",
       "       -0.00753283,  0.00597684, -0.00492334,  0.00601487, -0.02451228,\n",
       "       -0.03473803, -0.02589331,  0.01024564, -0.07351827,  0.00156897,\n",
       "       -0.0010245 , -0.04281166,  0.00118854,  0.07343443,  0.02192679,\n",
       "        0.00280779,  0.00762771,  0.06936575,  0.02672772,  0.04465892,\n",
       "       -0.01736239,  0.0089568 ,  0.0317954 ,  0.00314417, -0.03828576,\n",
       "        0.00504153,  0.04365131,  0.01730658,  0.00483506, -0.0161476 ,\n",
       "        0.01629756, -0.02420498,  0.04093078, -0.08007986, -0.04966146,\n",
       "        0.05359836,  0.01436524,  0.02558608, -0.07525507, -0.02119473,\n",
       "       -0.00691275,  0.00824482,  0.01338781, -0.01631197, -0.04553133,\n",
       "       -0.00383038, -0.01760913, -0.04402646, -0.02730542, -0.06043813,\n",
       "       -0.05289589,  0.03311013,  0.02871106,  0.00402558,  0.00787717,\n",
       "        0.00631431, -0.00242078,  0.01486983,  0.03708914,  0.0061563 ,\n",
       "       -0.01959556, -0.02824536,  0.00793712,  0.02638783,  0.02235305,\n",
       "        0.04178921,  0.01907887,  0.02657252, -0.01196242, -0.0012745 ,\n",
       "        0.05817388,  0.03762825,  0.05630253,  0.0025601 ,  0.04819488,\n",
       "        0.00687416,  0.00075248, -0.04716576, -0.01756615, -0.05071667,\n",
       "        0.02724582,  0.00688989, -0.0833609 , -0.01448378, -0.03777998,\n",
       "       -0.03201141, -0.02677369,  0.01392691, -0.01796267, -0.04272393,\n",
       "        0.03477356,  0.01262786,  0.01625196,  0.00800432,  0.02059877,\n",
       "        0.00029042, -0.06101388,  0.01004615, -0.04170983, -0.01021651,\n",
       "       -0.00450376,  0.03546827, -0.01988182, -0.0120099 ,  0.02120734,\n",
       "        0.04956903,  0.00104268,  0.0372365 ,  0.03826442,  0.00219027,\n",
       "       -0.07263774, -0.0337206 ,  0.00217977,  0.02688723, -0.02476322,\n",
       "       -0.00420604, -0.04098465, -0.01156475,  0.01816486, -0.03260615,\n",
       "       -0.00471864,  0.02179189,  0.01254132,  0.00659491, -0.00276229,\n",
       "       -0.07078113,  0.0383008 , -0.01958979,  0.01218352,  0.02091293,\n",
       "        0.01537516, -0.01569794,  0.00148774,  0.05515485,  0.00387147,\n",
       "        0.01115903, -0.03316785,  0.03970306,  0.00394214, -0.01486006,\n",
       "        0.03859279,  0.025606  ,  0.0097163 , -0.03545687,  0.02827473,\n",
       "        0.00047136, -0.0217933 , -0.0024085 ,  0.02607168,  0.00109043,\n",
       "        0.02867617,  0.00664367, -0.00123873, -0.04326825,  0.02302493,\n",
       "        0.00844341, -0.02510209, -0.00048964,  0.03398999, -0.05010309,\n",
       "       -0.02363463,  0.05811525,  0.01898168, -0.00386457, -0.01750914,\n",
       "       -0.002876  ,  0.03581462,  0.00487983, -0.00494983,  0.02007022,\n",
       "       -0.00164184,  0.02008011,  0.00670284,  0.0435237 , -0.00186444,\n",
       "        0.02510838,  0.01738377, -0.05396909,  0.00215204, -0.0114636 ,\n",
       "       -0.0057573 , -0.0074134 ,  0.00561436, -0.03265997,  0.02122883,\n",
       "        0.06953749,  0.009148  , -0.00541581,  0.01676721,  0.03564115,\n",
       "       -0.02121138, -0.01828679, -0.03038941, -0.03967563,  0.01528369,\n",
       "       -0.04153692,  0.02210303, -0.00759469,  0.0257625 ,  0.00917491,\n",
       "       -0.02705255,  0.00278201,  0.03622792,  0.02851611,  0.00054143,\n",
       "        0.0383883 ,  0.05754366, -0.03804975,  0.05450045,  0.01885366,\n",
       "        0.01401661,  0.0044676 ,  0.01133715,  0.01285562,  0.00676553,\n",
       "       -0.0108258 , -0.04496206, -0.04129516, -0.05672259, -0.01286313],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `infer_vector()` takes a list of *string tokens*\n",
    "* Input should be tokenized prior to inference\n",
    "    * Here the test set is already tokenized (in `test_corpus = list(read_corpus(test_data, tokens_only=True))`)\n",
    "    \n",
    "Note: algorithms use internal randomization, so repeated inferences of the same text will return slightly different vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "Deleting training data from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"reddit-doc2vec.model\")\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load the model:\n",
    "\n",
    "`model = Doc2Vec.load(fname)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To use model for inference:\n",
    "\n",
    "`vector = model.infer_vector([\"tokenized\", \"input\", \"string\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "### To load the model:\n",
    "* `model = Doc2Vec.load(fname)` (not required here)\n",
    "### To use model for inference:\n",
    "* `vector = model.infer_vector([\"tokenized\", \"input\", \"string\"])`\n",
    "    #### To tokenize:\n",
    "    * `list(read_corpus(df, tokens_only=False))` (used earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized test sample:\n",
      "[]\n",
      "\n",
      "Inferred vector:\n",
      "[ 9.76270094e-05  4.30378743e-04  2.05526754e-04  8.97663631e-05\n",
      " -1.52690394e-04  2.91788223e-04 -1.24825572e-04  7.83546013e-04\n",
      "  9.27325513e-04 -2.33116967e-04  5.83450077e-04  5.77898390e-05\n",
      "  1.36089118e-04  8.51193268e-04 -8.57927895e-04 -8.25741387e-04\n",
      " -9.59563185e-04  6.65239699e-04  5.56313491e-04  7.40024319e-04\n",
      "  9.57236683e-04  5.98317129e-04 -7.70412735e-05  5.61058347e-04\n",
      " -7.63451157e-04  2.79842032e-04 -7.13293441e-04  8.89337854e-04\n",
      "  4.36966438e-05 -1.70676125e-04 -4.70888772e-04  5.48467389e-04\n",
      " -8.76993363e-05  1.36867893e-04 -9.62420425e-04  2.35271000e-04\n",
      "  2.24191448e-04  2.33867991e-04  8.87496164e-04  3.63640604e-04\n",
      " -2.80984212e-04 -1.25936087e-04  3.95262381e-04 -8.79549072e-04\n",
      "  3.33533419e-04  3.41275736e-04 -5.79234853e-04 -7.42147386e-04\n",
      " -3.69143294e-04 -2.72578473e-04  1.40393546e-04 -1.22796977e-04\n",
      "  9.76747717e-04 -7.95910368e-04 -5.82246459e-04 -6.77380944e-04\n",
      "  3.06216651e-04 -4.93416796e-04 -6.73784525e-05 -5.11148828e-04\n",
      " -6.82060840e-04 -7.79249705e-04  3.12659191e-04 -7.23634090e-04\n",
      " -6.06835296e-04 -2.62549671e-04  6.41986437e-04 -8.05797463e-04\n",
      "  6.75889838e-04 -8.07803182e-04  9.52918956e-04 -6.26975961e-05\n",
      "  9.53522162e-04  2.09691047e-04  4.78527159e-04 -9.21624422e-04\n",
      " -4.34386078e-04 -7.59606890e-04 -4.07719606e-04 -7.62544572e-04\n",
      " -3.64033651e-04 -1.71474006e-04 -8.71705008e-04  3.84944229e-04\n",
      "  1.33202906e-04 -4.69221006e-04  4.64961086e-05 -8.12118989e-04\n",
      "  1.51892993e-04  8.58592393e-04 -3.62862105e-04  3.34820768e-04\n",
      " -7.36404269e-04  4.32654400e-04 -4.21187811e-04 -6.33617281e-04\n",
      "  1.73025866e-04 -9.59784898e-04  6.57880038e-04 -9.90609056e-04\n",
      "  3.55633063e-04 -4.59984061e-04  4.70388040e-04  9.24377062e-04\n",
      " -5.02493698e-04  1.52314664e-04  1.84083867e-04  1.44503807e-04\n",
      " -5.53836755e-04  9.05498047e-04 -1.05749241e-04  6.92817324e-04\n",
      "  3.98958538e-04 -4.05126106e-04  6.27595640e-04 -2.06988523e-04\n",
      "  7.62206386e-04  1.62545752e-04  7.63470714e-04  3.85063177e-04\n",
      "  4.50508553e-04  2.64876394e-06  9.12167248e-04  2.87980394e-04\n",
      " -1.52289897e-04  2.12786428e-04 -9.61613609e-04 -3.96850373e-04\n",
      "  3.20347084e-04 -4.19844786e-04  2.36030857e-04 -1.42462595e-04\n",
      " -7.29051884e-04 -4.03435348e-04  1.39929820e-04  1.81745520e-04\n",
      "  1.48650492e-04  3.06401635e-04  3.04206536e-04 -1.37163122e-04\n",
      "  7.93093175e-04 -2.64876260e-04 -1.28270156e-04  7.83846714e-04\n",
      "  6.12387958e-04  4.07777174e-04 -7.99546251e-04  8.38965236e-04\n",
      "  4.28482599e-04  9.97693976e-04 -7.01103418e-04  7.36252114e-04\n",
      " -6.75014104e-04  2.31119135e-04 -7.52360036e-04  6.96016476e-04\n",
      "  6.14637916e-04  1.38201474e-04 -1.85633398e-04 -8.61665991e-04\n",
      "  3.94857547e-04 -9.29146336e-05  4.44111211e-04  7.32764660e-04\n",
      "  9.51043039e-04  7.11606699e-04 -9.76571813e-04 -2.80043867e-04\n",
      "  4.59981122e-04 -6.56740624e-04  4.20732140e-05 -8.91324016e-04\n",
      " -6.00006955e-04 -9.62956401e-04  5.87395392e-04 -5.52150595e-04\n",
      " -3.09296651e-04  8.56162573e-04  4.08828811e-04 -9.36322147e-04\n",
      " -6.70611684e-04  2.42956798e-04  1.54457171e-04 -5.24214352e-04\n",
      "  8.68427975e-04  2.27931916e-04  7.12656038e-05  1.79819952e-04\n",
      "  4.60244046e-04 -3.76110023e-04 -2.03557880e-04 -5.80312510e-04\n",
      " -6.27613976e-04  8.88744777e-04  4.79101582e-04 -1.90823830e-05\n",
      " -5.45170740e-04 -4.91287035e-04 -8.83941655e-04 -1.31166744e-04\n",
      " -3.76408250e-04  3.92686983e-04 -2.44496332e-04 -6.40792656e-04\n",
      " -9.50642570e-04 -8.65500711e-04  3.58785561e-04 -9.26063076e-05\n",
      "  7.31584223e-05  7.93342595e-04  9.80677898e-04 -5.66206058e-04\n",
      "  3.26156412e-04 -4.73355234e-04 -9.58697987e-04  5.16757311e-04\n",
      " -3.59965692e-04 -2.33072205e-04  1.76634232e-04  6.62096892e-04\n",
      "  2.57963693e-04  7.45301310e-04 -4.52915934e-04  5.96093654e-04\n",
      " -6.28728129e-04  9.05583322e-04  3.74976546e-04 -5.68984658e-04\n",
      "  8.94741155e-04  4.61711606e-04 -4.92116727e-04 -5.73376019e-04\n",
      "  3.64014268e-05 -9.48674569e-04 -5.85059868e-04 -1.50629057e-04\n",
      " -2.51660036e-04 -7.28491505e-05 -4.44742589e-04  1.73568696e-04\n",
      "  7.27711187e-04 -7.64936267e-04  3.47582136e-05 -7.35863810e-04\n",
      "  4.33719368e-04 -2.07880599e-04  1.30842629e-04 -6.33440330e-04\n",
      " -7.10304477e-04 -2.38874381e-05 -2.88774521e-04  8.80863867e-04\n",
      "  5.30650490e-04  4.97327244e-04  8.07439501e-04 -8.33155122e-04\n",
      "  1.04384941e-04  1.68952145e-04  9.23872751e-04 -4.15704941e-04\n",
      " -5.18342422e-04 -7.99412141e-04 -9.67140717e-04  8.59058637e-04\n",
      "  3.39833088e-04  5.70305798e-04 -4.36539791e-04  1.72820335e-04\n",
      " -8.72089469e-04 -2.87448074e-05  9.54990275e-04  7.53010507e-04\n",
      " -3.23682092e-04  9.23140324e-04 -5.36596752e-04  8.98637634e-04\n",
      "  8.82755383e-04  5.98405197e-04  2.60895875e-04  7.48575956e-04\n",
      " -4.13959438e-04  6.97887095e-04  2.35753381e-04 -9.73526272e-04\n",
      " -3.05532973e-04 -7.03718280e-04  9.63658793e-04 -4.32593843e-05\n",
      " -5.21726906e-06  2.78945023e-04 -2.62830785e-04 -7.26199476e-04\n",
      "  6.44235464e-04 -6.20304199e-04  2.26379652e-05 -5.51365956e-04\n",
      " -8.04311014e-04  7.24383048e-04  9.45838983e-04  9.21669300e-04\n",
      "  8.13111023e-04  5.48094686e-04 -3.33709700e-04 -8.37797241e-04\n",
      " -1.85517652e-04 -5.35531726e-04 -7.35024747e-04 -8.93145625e-04\n",
      "  4.51188738e-04 -9.77145042e-04  5.41161513e-04 -7.06106715e-04\n",
      " -8.40955821e-04 -8.20793910e-04  3.44095606e-04 -5.09265577e-04\n",
      " -1.58921073e-04  1.14737581e-04  7.21102348e-04  4.54088527e-04\n",
      " -4.59344184e-04 -7.37034425e-04 -8.89251358e-04 -3.96802730e-04\n",
      " -4.75763693e-04 -8.77188650e-05  3.66562657e-04  3.91250884e-04\n",
      " -4.32962319e-04 -2.40146095e-04 -6.37698104e-04  5.77091007e-04\n",
      " -8.86303838e-04  3.93994473e-04  5.57390798e-04  5.54815109e-04\n",
      " -4.81154857e-04 -2.52373720e-04  1.75199268e-04 -4.54356195e-04\n",
      " -2.58294400e-04 -6.05891459e-04 -8.02882350e-05 -9.10775387e-04\n",
      "  5.99591760e-04 -8.46087118e-04  3.76702992e-05 -3.86379805e-04\n",
      "  1.55085901e-04  9.18866659e-04  2.91140488e-04 -9.29275120e-04\n",
      " -1.39195123e-04  2.00337054e-05  7.23549892e-05  3.62785009e-04\n",
      " -4.44807811e-04 -7.42278877e-04 -2.14648651e-04  9.12811432e-04\n",
      " -6.25738234e-04  8.07967910e-04  8.76119011e-05 -8.61771550e-05\n",
      "  7.64082826e-04 -8.27920740e-05  4.48335282e-04 -2.01949355e-04\n",
      "  8.08088807e-04  3.80050042e-04  3.99244105e-04 -3.44559201e-04\n",
      "  5.13557286e-04  2.72122124e-04 -5.19959431e-04 -6.78922341e-04\n",
      "  5.92782977e-04  9.18333186e-04 -8.37223488e-05  1.81968324e-04\n",
      "  7.15445261e-04 -8.55530961e-05  9.03748965e-04  1.51502318e-04\n",
      "  6.41534221e-04  8.17687425e-04  6.31047646e-04 -6.81171077e-04\n",
      "  2.57796870e-04 -2.03131480e-04 -8.74574122e-04 -1.51935499e-04\n",
      " -4.82631876e-04  6.98076619e-04 -9.33390751e-04  9.17965430e-04\n",
      " -2.89262302e-04 -2.86586233e-04 -9.67342989e-04 -6.29535352e-04\n",
      " -1.97481000e-04  8.58582847e-04 -8.00770125e-04  8.90603056e-04\n",
      "  7.38977047e-04 -9.16752033e-05 -3.46598245e-04 -5.34511753e-04\n",
      "  2.28929406e-04 -9.33850824e-04 -9.68787877e-04 -1.42408549e-04\n",
      " -8.63851863e-04 -4.96118038e-04 -5.57678170e-04 -4.93617612e-04\n",
      " -7.37889553e-04 -9.75927571e-04 -7.69031409e-04  2.36960521e-04\n",
      "  9.48512403e-04  9.80690005e-04 -1.81891810e-04 -6.74091163e-04\n",
      "  2.77523504e-04 -1.93893065e-05  9.78819560e-04 -8.69391602e-04\n",
      "  5.66468865e-04 -4.23203019e-04 -5.17162785e-04  3.25009139e-04\n",
      " -5.07873658e-04  3.31718242e-04  3.46170345e-05 -1.51822023e-04\n",
      "  1.09375615e-04 -4.25896957e-04  4.13149421e-04 -1.70286265e-04\n",
      " -2.78908876e-04  6.57313853e-04  8.49933829e-04 -9.07985377e-04\n",
      " -5.34746039e-04 -3.02961271e-04  6.29932969e-04  9.70982830e-04\n",
      "  9.37943405e-04  8.09896679e-04 -4.06887470e-04  9.84022510e-04\n",
      " -5.01159928e-04 -7.88187666e-04  9.01905238e-04 -5.33159473e-04\n",
      "  3.79536534e-04 -8.83287285e-04  4.61418211e-04  7.63440446e-04\n",
      " -4.55126195e-04 -2.41886213e-04 -2.51407648e-04  4.97576490e-04\n",
      " -5.24385541e-04 -6.56293822e-04 -1.01416699e-04 -3.91063193e-04\n",
      "  6.78378216e-04 -5.24516334e-04  4.77891490e-06  8.85167217e-04\n",
      "  2.67995405e-04  7.34578818e-04  8.80419393e-04  5.01529721e-04\n",
      "  3.99150129e-04  9.35931108e-04  9.88801592e-04 -9.63566345e-05\n",
      " -8.58260435e-04 -4.14411945e-04 -6.95290568e-04 -1.65027246e-04\n",
      " -7.37421331e-04  2.08235608e-04 -2.34383886e-04  7.90771795e-04\n",
      "  9.35589371e-04  9.37698060e-05 -4.50352847e-04  1.84460834e-04\n",
      "  7.93522340e-04 -1.86533303e-04  1.04156556e-04 -4.56694455e-04\n",
      " -8.91117015e-05 -1.96572932e-04 -5.03173098e-04  1.17327672e-05]\n"
     ]
    }
   ],
   "source": [
    "vector_sample = model.infer_vector(test_corpus[1])\n",
    "print(\"Tokenized test sample:\")\n",
    "print(test_corpus[1])\n",
    "print(\"\\nInferred vector:\")\n",
    "print(vector_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For more information:\n",
    "* [Yaron Vazana](http://yaronvazana.com/2018/01/20/training-doc2vec-model-with-gensim/)\n",
    "* [Rare Technologies](https://rare-technologies.com/doc2vec-tutorial/)\n",
    "* [Gensim Documentation](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "* [Gensim Doc2Vec Tutorial on the IMDB Sentiment Dataset](./doc2vec-IMDB.ipynb)\n",
    "* [Doc2Vec Tutorial on the Lee Dataset](./Doc2Vec_Tutorial_Lee_mod.ipynb)\n",
    "* [Doc2Vec Testing Trained Model](./Doc2Vec_Test_Lee.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
