{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Train and Test\n",
    "The idea is to ultimately create a module that takes the data frame and return, instead of the body text, retuns a vector for each paragraph input (data input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train and test data\n",
    "\n",
    "* Data (train and test): [Cleaned reddit dataset](../../data/ad_hominem/ad_hominems_cleaned.csv), the data will be separated into test and train in a 70-30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "data = pd.read_csv(\"../../data/ad_hominem/ad_hominems_cleaned.csv\")\n",
    "train_data, test_data = np.split(data.sample(frac=1), [int(.7*len(data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Read and Preprocess Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a function to open the train/test file (with latin encoding)\n",
    "* Read the file line-by-line\n",
    "* Pre-process each line using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "* Return a list of words.\n",
    "Note that, for the data frame (corpus), each row constitutes a single document and the length of row entry (i.e., document) can vary. Also, to train the model, we'll need to associate a tag/number with each document of the training corpus. In our case, the tag is simply the index for the data frame (row number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(df, tokens_only=False):\n",
    "    #with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "    for i, line in df.iterrows():\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(str(line[\"reddit_ad_hominem.body\"]))\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(str(line[\"reddit_ad_hominem.body\"])), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(train_data))\n",
    "test_corpus = list(read_corpus(test_data, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus (both in the data frame and the generated corpus to see the differences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_ad_hominem.body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50255</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      reddit_ad_hominem.body\n",
       "50255  NaN                  \n",
       "99976  NaN                  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.DataFrame(train_data.loc[:, \"reddit_ad_hominem.body\"])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['nan'], tags=[50255]),\n",
       " TaggedDocument(words=['nan'], tags=[99976])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_ad_hominem.body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79142</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69811</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      reddit_ad_hominem.body\n",
       "79142  NaN                  \n",
       "69811  NaN                  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_data.loc[:, \"reddit_ad_hominem.body\"])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nan'], ['nan']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain any tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "### Instantiate a Doc2Vec Object \n",
    "Doc2Vec model with:\n",
    "* Vector size with 500 words\n",
    "* Iterating over the training corpus 10 times (More iterations take more time and eventually reach a point of diminishing returns)\n",
    "* Minimum word count set to 20 (discard words with very few occurrences)\n",
    "\n",
    "Note: retaining infrequent words can often make a model worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "  model = gensim.models.doc2vec.Doc2Vec(vector_size=500, min_count=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via `model.wv.vocab`) of all of the unique words extracted from the training corpus along with the count (e.g., `model.wv.vocab['penalty'].count` for counts for the word `penalty`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 29s, sys: 4.86 s, total: 3min 34s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vector can then be compared with other vectors via cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03302052, -0.00475105,  0.01161314,  0.0236905 , -0.0537018 ,\n",
       "       -0.00255536,  0.01054048,  0.02972727,  0.0154462 ,  0.02121845,\n",
       "        0.00277529, -0.0707717 , -0.01914096, -0.04099298,  0.01934998,\n",
       "        0.02442866,  0.03670777, -0.00923106, -0.01774863, -0.02243855,\n",
       "       -0.04781026, -0.09083778,  0.04270164, -0.01088333, -0.05486543,\n",
       "       -0.02477331,  0.00861344,  0.00278031, -0.00727144, -0.02567455,\n",
       "       -0.0065259 , -0.04000838,  0.00248543, -0.06729561, -0.04923471,\n",
       "       -0.03039499, -0.01674587, -0.02626782, -0.06380375,  0.02066823,\n",
       "       -0.03653348, -0.00031122,  0.0093143 , -0.06327739, -0.03727213,\n",
       "        0.01684709,  0.01302233, -0.00377187, -0.02742417,  0.00583922,\n",
       "        0.03568843,  0.00695386, -0.00476701,  0.00950964,  0.01364512,\n",
       "        0.01799491, -0.02467484, -0.03712162, -0.0182891 , -0.01591285,\n",
       "       -0.02610657,  0.0193175 , -0.00923604,  0.00321477,  0.01475343,\n",
       "        0.00436636,  0.02516589,  0.01650405,  0.00501648,  0.00667417,\n",
       "        0.06338306,  0.00057955,  0.0405003 ,  0.06382018,  0.03004134,\n",
       "        0.09389836, -0.04505996,  0.0717743 ,  0.01132599, -0.03123734,\n",
       "       -0.0223319 ,  0.03452578, -0.00893009,  0.0028853 ,  0.02092548,\n",
       "       -0.00356485,  0.01343087,  0.03090011, -0.00792114,  0.01364698,\n",
       "        0.02438881,  0.04094058, -0.0006847 ,  0.01892432, -0.02378573,\n",
       "       -0.04204941, -0.04358546,  0.00369228,  0.01470569, -0.0081597 ,\n",
       "       -0.04419959, -0.02801601, -0.01320572,  0.01134578,  0.06956382,\n",
       "        0.02966668, -0.02160286, -0.01768304, -0.06792618,  0.06779625,\n",
       "        0.01418706,  0.044903  ,  0.01167383, -0.00083671,  0.01770668,\n",
       "        0.02364944, -0.03006202, -0.03018856,  0.00638617, -0.00485007,\n",
       "        0.01733411,  0.00231398,  0.04649723,  0.04382604, -0.02604234,\n",
       "        0.00780269,  0.00964337, -0.01523535, -0.02294432,  0.03289979,\n",
       "       -0.02608653,  0.02765416, -0.01198219, -0.03680393,  0.02950057,\n",
       "       -0.05499072,  0.07774577, -0.06877253, -0.01800889, -0.00248154,\n",
       "       -0.00795156, -0.0065169 , -0.00627136,  0.00706926, -0.01671148,\n",
       "        0.00601947, -0.00282854, -0.00850524,  0.07168159, -0.01021631,\n",
       "       -0.01852837, -0.00386708, -0.05305092, -0.00886373, -0.07825418,\n",
       "       -0.02372355,  0.0171522 ,  0.02371185, -0.06184977,  0.01504327,\n",
       "       -0.02284617, -0.02998516, -0.01941318,  0.01448292,  0.00509707,\n",
       "       -0.03224092, -0.01675546,  0.06178088,  0.0183305 ,  0.00971715,\n",
       "        0.02095702, -0.02368353,  0.01524136,  0.05358256,  0.0408215 ,\n",
       "        0.01465172, -0.02280851, -0.02910092, -0.01361095,  0.03293959,\n",
       "        0.00394155,  0.04317545,  0.00873013, -0.01664962,  0.05756762,\n",
       "        0.02913705,  0.01899662, -0.01925825,  0.01467572,  0.01456079,\n",
       "        0.04364382, -0.03529372, -0.01742289,  0.06240363,  0.08649155,\n",
       "       -0.01333412, -0.00667482, -0.0181892 ,  0.02803235,  0.02946094,\n",
       "        0.01396856, -0.04139686, -0.04587866,  0.03517972, -0.06404217,\n",
       "       -0.04120291,  0.01115514, -0.01223389,  0.03792683,  0.00124128,\n",
       "       -0.03649472,  0.05084764,  0.06097407,  0.02004213,  0.00413855,\n",
       "       -0.02552074, -0.02077946,  0.03306108, -0.05696856, -0.11790714,\n",
       "        0.03815352, -0.03660582,  0.01660543,  0.0133536 ,  0.00771723,\n",
       "       -0.00956182,  0.02335788,  0.04699478, -0.00505835,  0.02476327,\n",
       "       -0.00394767, -0.07591464, -0.00895202,  0.07484803, -0.04173605,\n",
       "       -0.0595279 , -0.03475889, -0.01178097, -0.0699251 ,  0.03487829,\n",
       "       -0.06761292,  0.02551533, -0.02168931, -0.01290996, -0.03222629,\n",
       "        0.00756291, -0.05297972, -0.02462774, -0.00197993,  0.00218555,\n",
       "        0.06986964,  0.00636946,  0.01774098,  0.06376614, -0.03374619,\n",
       "       -0.00528963, -0.0544901 , -0.00746483, -0.02099477,  0.00965002,\n",
       "        0.0457393 ,  0.02103425, -0.06394503, -0.03364848,  0.01438367,\n",
       "        0.01417507, -0.00424602, -0.07450143, -0.0254914 , -0.00173169,\n",
       "        0.02890646, -0.01885495, -0.01164206,  0.00982404,  0.02842138,\n",
       "        0.02729098, -0.01543999,  0.01934126, -0.06092254,  0.00032315,\n",
       "        0.04595143, -0.03777843,  0.00283768,  0.01979257, -0.04532693,\n",
       "        0.03768665, -0.04572831, -0.02011497,  0.04595864, -0.04485968,\n",
       "        0.0016704 ,  0.05266528,  0.03805068, -0.00292415, -0.03595625,\n",
       "       -0.05673853,  0.03431814, -0.02096507, -0.03104262,  0.01900491,\n",
       "       -0.0225874 ,  0.01716765,  0.02023315,  0.03394691,  0.02712994,\n",
       "        0.03508312,  0.04157301,  0.01269582, -0.01512951,  0.00282527,\n",
       "       -0.01070434, -0.00132491, -0.03150841,  0.02462093, -0.03839892,\n",
       "       -0.04748407,  0.03465724, -0.01819864,  0.01216657,  0.03818755,\n",
       "        0.06410453, -0.01074689,  0.0107548 , -0.0147546 ,  0.00013755,\n",
       "        0.0168827 ,  0.05436503,  0.04341717,  0.00156888,  0.00919414,\n",
       "       -0.00373348,  0.03674953,  0.07111163,  0.06069108, -0.04531087,\n",
       "       -0.01944871, -0.00175777, -0.06207011,  0.05148149, -0.00556836,\n",
       "        0.00128356,  0.01437773,  0.03449332,  0.0231241 , -0.00775734,\n",
       "       -0.06390595, -0.04787854, -0.02447135, -0.01501493,  0.01590323,\n",
       "        0.01985252,  0.01694216,  0.01393547,  0.0065188 , -0.01869171,\n",
       "        0.01473793, -0.03434157,  0.02589407, -0.08420265, -0.08811393,\n",
       "       -0.00355988, -0.00206622, -0.0014809 ,  0.01582524,  0.03017256,\n",
       "        0.02994417,  0.00646495,  0.05667542, -0.0040395 , -0.00755066,\n",
       "       -0.04266668,  0.04589322,  0.01799885,  0.02751733, -0.0357412 ,\n",
       "       -0.01900431,  0.03563919,  0.0268675 , -0.02504522,  0.02806028,\n",
       "       -0.01455849, -0.03723303,  0.04503477, -0.02156515, -0.02184042,\n",
       "        0.00476298, -0.04718948, -0.02730149,  0.00047049,  0.0167293 ,\n",
       "        0.01937109,  0.01981844,  0.0128492 ,  0.0078205 ,  0.04958102,\n",
       "       -0.01549473,  0.06928211,  0.02487615, -0.02006485,  0.03442974,\n",
       "        0.04126428,  0.00264431, -0.01278527, -0.04313006,  0.01166815,\n",
       "       -0.01617184,  0.01562213, -0.00167291,  0.04194205,  0.05114052,\n",
       "        0.03538062, -0.10560854,  0.01560001, -0.04166584, -0.0206187 ,\n",
       "       -0.04745313,  0.0237753 , -0.05642546,  0.02799554,  0.07323323,\n",
       "        0.00561762, -0.04322425, -0.00327427, -0.00175215,  0.0251221 ,\n",
       "        0.02685166, -0.0297966 , -0.01293539,  0.00446435, -0.07379464,\n",
       "        0.01171202,  0.01092273,  0.02192195,  0.02329854, -0.02279336,\n",
       "        0.02709532, -0.00918756,  0.02175322, -0.02704031, -0.00734691,\n",
       "       -0.01664948,  0.00255794, -0.04774718,  0.00319285, -0.06288572,\n",
       "        0.02820839,  0.05128167, -0.0046939 , -0.00794075,  0.02189435,\n",
       "       -0.06346832, -0.05359768, -0.07453568,  0.04512087, -0.00445517,\n",
       "        0.03347114, -0.01820511,  0.11438026, -0.03158931, -0.02917418,\n",
       "        0.02794749, -0.00993167, -0.03116614,  0.008871  ,  0.00910959,\n",
       "       -0.07824781, -0.02187698,  0.03150716,  0.01697711, -0.03247575,\n",
       "       -0.04190237, -0.01710252, -0.00472537,  0.01128409,  0.00564165,\n",
       "        0.00184927,  0.05208876,  0.07255238, -0.0107554 , -0.03393888,\n",
       "       -0.0003614 ,  0.00166091,  0.0071574 , -0.03687382,  0.04309161,\n",
       "        0.01053234,  0.02230597,  0.01382509,  0.03044972, -0.0059812 ,\n",
       "        0.05459914, -0.00095684,  0.05710676, -0.01961531,  0.00837514,\n",
       "        0.03697209,  0.0065978 ,  0.00743765,  0.01786403,  0.00245279],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `infer_vector()` takes a list of *string tokens*\n",
    "* Input should be tokenized prior to inference\n",
    "    * Here the test set is already tokenized (in `test_corpus = list(read_corpus(test_data, tokens_only=True))`)\n",
    "    \n",
    "Note: algorithms use internal randomization, so repeated inferences of the same text will return slightly different vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "Deleting training data from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"reddit-doc2vec.model\")\n",
    "#model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load the model:\n",
    "\n",
    "`model = Doc2Vec.load(fname)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To use model for inference:\n",
    "\n",
    "`vector = model.infer_vector([\"tokenized\", \"input\", \"string\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "### To load the model:\n",
    "* `model = Doc2Vec.load(fname)` (not required here)\n",
    "### To use model for inference:\n",
    "* `vector = model.infer_vector([\"tokenized\", \"input\", \"string\"])`\n",
    "    #### To tokenize:\n",
    "    * `list(read_corpus(df, tokens_only=False))` (used earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized test sample:\n",
      "['nan']\n",
      "\n",
      "Inferred vector:\n",
      "[ 1.60084721e-02  1.02460361e-03 -1.89598370e-02  7.23619899e-03\n",
      " -1.24632930e-02 -5.63005311e-03  1.60672376e-03  1.42009286e-02\n",
      "  1.64097082e-02  1.69727746e-02  5.79811260e-03 -1.75342038e-02\n",
      "  3.11491662e-03 -1.05276387e-02  2.83717224e-03 -7.69761810e-03\n",
      "  4.51505510e-03  3.55933816e-03  1.06733304e-03 -2.52480921e-03\n",
      " -1.45907858e-02 -1.14028715e-02 -3.49434535e-03  8.23463965e-03\n",
      " -8.31985194e-03 -5.32701332e-03  1.74424239e-02  1.08821699e-02\n",
      "  9.20679234e-03  1.74813031e-03  3.04745976e-03 -1.13345059e-02\n",
      " -1.58613976e-02  2.52946839e-03 -1.96615309e-02 -8.69426318e-03\n",
      "  1.80128578e-03 -1.79972837e-03 -3.56873032e-03 -6.59934292e-03\n",
      " -2.12082453e-02 -1.64342839e-02 -2.37535196e-03  1.06621720e-03\n",
      " -1.69275738e-02  2.13032681e-02 -5.35039324e-03  7.81056518e-03\n",
      " -2.80448049e-03 -9.15284827e-03  5.72796864e-03  5.66163892e-03\n",
      "  5.99653134e-03 -9.80296265e-03  8.81322194e-03 -7.95341213e-04\n",
      " -4.51059034e-03 -9.75381769e-03 -6.96850056e-03  5.83844492e-03\n",
      " -3.20195523e-03  4.27187188e-03 -5.46027394e-03 -3.27480375e-03\n",
      " -9.80120338e-03 -1.24839600e-02  1.46080581e-02  2.48886785e-03\n",
      "  1.37966324e-03 -1.16689447e-02  1.26961456e-03 -1.02251498e-02\n",
      "  1.19946580e-02  1.53118335e-02  5.78159373e-03  3.47841741e-03\n",
      " -1.94916583e-03  7.27684516e-03  1.64603442e-03  7.90250115e-03\n",
      " -7.78703345e-03  1.46833621e-02 -1.53801944e-02  5.90896979e-03\n",
      "  1.85980201e-02  1.31268166e-02  1.31537835e-03 -5.22526447e-04\n",
      "  1.82536319e-02 -8.33088346e-03  1.77734613e-03 -5.00892522e-03\n",
      " -6.45816233e-03 -8.82867351e-03 -8.85454193e-03 -5.12661086e-03\n",
      " -3.54974344e-03  3.13627953e-03 -7.61310046e-04 -1.27901032e-03\n",
      " -1.03416631e-03 -9.12599172e-03  8.80971365e-03  9.44798067e-03\n",
      "  6.87270472e-03  8.17438681e-03 -1.73828867e-03 -1.09741725e-02\n",
      " -2.44698394e-03  1.51739763e-02  6.42427849e-03  1.49219185e-02\n",
      "  5.90607012e-03 -3.43294698e-03  6.06878567e-03  4.01957333e-03\n",
      "  7.90124480e-03 -1.32263070e-02  5.81777003e-03  1.47361122e-03\n",
      " -3.23081366e-03  4.25335206e-03  6.38865959e-03  6.55533094e-03\n",
      "  1.75516470e-03  8.70658143e-04  6.55856170e-03 -1.22225527e-02\n",
      " -1.06440317e-02  8.15944094e-03 -1.70355174e-03 -1.49239423e-02\n",
      " -6.68326439e-03 -9.71205439e-03  6.94534974e-03 -1.89206693e-02\n",
      "  2.40772348e-02 -1.57891810e-02 -9.26424190e-03 -7.48945680e-03\n",
      "  5.37646748e-03  1.75870047e-03  1.57927889e-02 -6.04539458e-03\n",
      "  1.04229664e-04  4.74927743e-04 -1.63884778e-02  1.39259398e-02\n",
      "  5.05937217e-03  8.61702580e-03  7.62076676e-03  7.30325328e-03\n",
      " -1.07251415e-02  9.00001323e-04 -1.57574555e-04  4.04407596e-03\n",
      "  1.19776884e-02 -1.04804095e-02  9.82449763e-03  1.25534181e-02\n",
      "  2.30322499e-03 -1.91594716e-02  5.93462819e-03  8.24899226e-03\n",
      "  5.73911006e-03 -3.64043890e-03 -4.30349400e-03 -5.28784003e-04\n",
      "  5.35898935e-03  2.84982543e-03  8.01314879e-03 -7.21677486e-03\n",
      " -1.34645053e-03  1.49710681e-02 -1.04754348e-03  1.51356058e-02\n",
      "  7.93154910e-03 -3.44749377e-03  4.21136292e-03  5.98217919e-03\n",
      " -5.26233437e-03  3.17378552e-03  3.73606337e-03 -2.04532436e-04\n",
      "  1.57495458e-02  2.90315668e-03  8.18093121e-03 -2.24820301e-02\n",
      " -1.01189315e-03  8.31530022e-04 -3.39205330e-03 -8.52416852e-05\n",
      " -1.07083907e-02  7.29014771e-03  2.15828810e-02  3.59735778e-03\n",
      "  7.95851927e-03 -2.64437567e-03 -2.24172650e-03 -1.44080142e-03\n",
      " -1.06227957e-03 -1.36617534e-02 -5.05894842e-03 -3.49260867e-03\n",
      " -4.82737971e-03 -9.30270739e-03  1.03878593e-02  1.07351085e-02\n",
      "  4.48232610e-03 -1.90399420e-02  6.38276245e-03 -2.94583105e-03\n",
      "  1.59179475e-02 -4.33039851e-03  1.49972341e-03 -8.61215871e-03\n",
      " -6.09611440e-03  5.80632128e-03 -4.51947330e-03 -2.13252511e-02\n",
      " -7.36376178e-03  3.75273405e-04 -3.49907530e-03  1.34246359e-02\n",
      " -9.39589925e-03 -3.22639919e-03  1.50887575e-03  1.18140550e-03\n",
      " -8.63413792e-03  3.96732800e-03 -1.77812111e-03 -2.01988989e-03\n",
      "  3.54325515e-03  5.89118747e-04 -1.06771262e-02 -1.57302357e-02\n",
      " -9.28798225e-03 -9.90268122e-03 -1.38522796e-02  8.35167710e-03\n",
      " -1.30666755e-02  1.07341716e-02 -3.72268376e-03 -1.14546972e-03\n",
      " -3.54051823e-03  1.09726312e-02  5.40210307e-03 -5.78934327e-03\n",
      "  2.26483098e-03 -5.05737588e-03  6.43494958e-03 -4.58007865e-03\n",
      "  1.63047295e-02 -9.84043814e-04 -1.71178498e-03 -2.02704803e-04\n",
      " -7.94949755e-03 -3.20962840e-03 -1.38324604e-03 -2.18602968e-03\n",
      " -3.65244574e-03  7.51123577e-03 -1.87491870e-03 -1.12533718e-02\n",
      " -4.53633349e-03 -1.12852398e-02  1.76970358e-03 -9.18282289e-03\n",
      " -4.90230648e-03 -5.44506416e-04  1.10206977e-02 -6.30387990e-03\n",
      "  3.82698840e-03 -9.20973066e-03 -8.27384763e-04  6.32602908e-03\n",
      " -9.18695983e-03  1.06457453e-02 -1.15073156e-02  7.53085082e-03\n",
      "  1.69769712e-02 -1.03481561e-02  9.27874446e-03 -1.30339805e-02\n",
      " -2.28130706e-02  1.15524530e-02 -1.36715807e-02  9.49043036e-03\n",
      "  7.87454285e-03  2.60888692e-03 -2.90783169e-03  1.97827630e-03\n",
      "  5.63572720e-03  2.81562586e-03 -5.17904107e-03 -1.62303541e-02\n",
      "  2.28003226e-02  6.64275931e-03 -6.95564551e-03  7.77241960e-03\n",
      " -8.56972393e-03 -6.31479127e-03  7.01651967e-04 -4.55788616e-03\n",
      " -3.58061981e-03  1.42476130e-02 -3.49442847e-03 -6.95346505e-04\n",
      " -1.03419852e-02 -1.22789359e-02 -1.11379549e-02  2.06619147e-02\n",
      " -1.07711973e-02 -7.42223626e-03 -5.25356689e-03 -1.11808423e-02\n",
      "  5.05784294e-03  9.25537082e-04  1.00711277e-02  1.07453410e-02\n",
      " -3.15214414e-03  1.16440793e-03  6.99922349e-03  8.39048065e-04\n",
      "  1.45361824e-02  1.46024453e-03  1.31256562e-02 -7.23805325e-03\n",
      " -5.57440217e-04 -4.52044362e-04  2.15216028e-03  4.65475675e-03\n",
      "  8.28910992e-03  7.00305961e-03 -1.99397691e-02  4.82858485e-03\n",
      "  4.46964428e-03 -9.99061484e-03 -2.53600348e-03 -1.39508047e-03\n",
      " -1.40803901e-03 -6.56734686e-03  1.01338616e-02 -7.88204651e-03\n",
      "  2.56457552e-03 -1.23226410e-02 -7.48791499e-03  5.95425081e-04\n",
      " -1.41661707e-02 -4.06371057e-03  1.22135561e-02  7.24206353e-03\n",
      "  5.63109526e-03  7.84638221e-04 -7.22814025e-03  3.74875171e-03\n",
      " -2.57177395e-03  4.84389439e-03 -1.47230434e-03 -7.68177817e-03\n",
      " -6.83502015e-03 -1.10940868e-02  6.07067207e-03  4.14728047e-03\n",
      "  2.24600174e-03 -2.45296455e-04  5.79308858e-03  2.27609929e-02\n",
      " -3.57393804e-03 -7.47001451e-03 -1.37277879e-02  6.10832358e-03\n",
      "  4.15699929e-03  1.64335817e-02  4.85980650e-03 -9.25119314e-03\n",
      " -2.11736915e-04  4.48567234e-03 -1.13584017e-02  6.95517147e-03\n",
      " -1.06281159e-03 -1.05293700e-02  1.90434773e-02 -1.41888112e-02\n",
      " -9.88968415e-04  4.32425272e-03 -8.50999914e-03  1.69184653e-03\n",
      " -6.34236168e-03  4.47275629e-03  7.39858253e-03  1.09981382e-02\n",
      "  9.58871190e-03  6.61722524e-03  5.36260341e-05 -1.12259053e-02\n",
      "  1.17193880e-02  8.87103658e-03 -1.49262531e-04  4.96000377e-03\n",
      "  2.20742077e-02  1.56217730e-02 -7.49748107e-03 -3.57029331e-03\n",
      "  8.31117295e-03 -6.72664912e-03  2.04213639e-03 -3.20810359e-05\n",
      "  7.60805421e-03 -2.96266540e-03  7.19232438e-03 -3.00880391e-02\n",
      "  4.98208683e-03 -1.80540769e-03 -1.20454477e-02 -4.98684077e-03\n",
      "  4.84184129e-03 -3.03113973e-03 -1.59782323e-03  1.11346394e-02\n",
      "  1.85443778e-02 -2.02940386e-02 -6.21919706e-03 -3.32918600e-04\n",
      "  1.50710847e-02  1.94790941e-02  2.05289398e-04  4.92556673e-03\n",
      " -9.99429543e-03 -2.34737936e-02 -2.03326298e-03  6.59987563e-06\n",
      "  7.36219902e-03 -4.89468407e-03 -8.46739858e-03 -8.17358308e-03\n",
      " -1.18641891e-02  4.70300019e-03  2.00789975e-04  8.93284380e-03\n",
      " -1.66374370e-02  1.06150005e-02 -1.96893793e-02 -1.68961764e-03\n",
      " -1.98820569e-02 -1.23246294e-02  6.18877495e-03  8.87252390e-03\n",
      "  5.63398004e-03  5.83439320e-03 -8.33474100e-03 -4.43414133e-03\n",
      " -2.37615071e-02  1.48452530e-02  9.84586426e-04  1.31278085e-02\n",
      "  5.11099584e-03  9.48189944e-03 -2.81854253e-03 -1.01946993e-02\n",
      "  6.67437958e-03  5.99735137e-03 -1.77516893e-04  1.30013900e-03\n",
      " -1.45121687e-03 -1.41088367e-02  9.65595338e-03  1.72414202e-02\n",
      "  2.89157890e-02 -5.45563875e-03 -4.04532673e-03 -1.77769247e-03\n",
      " -4.30082669e-03  9.28513799e-03  1.03888139e-02 -2.30991072e-03\n",
      "  4.82095266e-03  1.47327427e-02 -2.18542106e-03 -1.21284351e-02\n",
      "  4.55570081e-03  6.50411658e-03 -2.74366536e-03 -5.45772538e-03\n",
      " -2.53242906e-03  8.45547859e-03  7.76877278e-05 -3.77481338e-03\n",
      "  1.55348806e-02  1.04851164e-02  9.63182375e-03  1.73878241e-02\n",
      "  1.35869719e-02 -1.24580823e-02 -4.12657484e-03  6.61053322e-03\n",
      " -1.34948385e-03  9.86844022e-03 -1.40370789e-03  6.24969043e-03]\n"
     ]
    }
   ],
   "source": [
    "vector_sample = model.infer_vector(test_corpus[1])\n",
    "print(\"Tokenized test sample:\")\n",
    "print(test_corpus[1])\n",
    "print(\"\\nInferred vector:\")\n",
    "print(vector_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For more:\n",
    "* [Yaron Vazana](http://yaronvazana.com/2018/01/20/training-doc2vec-model-with-gensim/)\n",
    "* [Rare Technologies](https://rare-technologies.com/doc2vec-tutorial/)\n",
    "* [Gensim Documentation](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "* [Gensim Doc2Vec Tutorial on the IMDB Sentiment Dataset](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb)\n",
    "* [Doc2Vec Tutorial on the Lee Dataset](https://markroxor.github.io/gensim/static/notebooks/doc2vec-lee.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
